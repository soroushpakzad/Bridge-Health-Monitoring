{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxl0SysKwjxq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from scipy.stats import genpareto\n",
        "from sklearn.svm import OneClassSVM\n",
        "!pip install keras-tuner\n",
        "import kerastuner as kt  # make sure to install keras-tuner (pip install keras-tuner)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Load and Split the Dataset\n",
        "# =============================================================================\n",
        "# Load the full training data (Normal Condition)\n",
        "X_train_full = pd.read_excel(io=\"Freq_Z24N.xlsx\", sheet_name=\"Train Set\", header=0, engine=\"openpyxl\").values\n",
        "\n",
        "# Split the training set into training (80%) and validation (20%) sets\n",
        "X_train, X_val = train_test_split(X_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the test set, which remains unseen. It contains two parts:\n",
        "#   - first part: Inspection - Normal Condition (samples 1 to 864)\n",
        "#   - second part: Inspection - Damaged Condition (samples 865 to end)\n",
        "X_test_full = pd.read_excel(io=\"Freq_Z24N.xlsx\", sheet_name=\"Test Set\", header=0, engine=\"openpyxl\").values\n",
        "X_test_normal = X_test_full[:510]    # Inspection - Normal Condition\n",
        "X_test_damaged = X_test_full[510:]     # Inspection - Damaged Condition\n",
        "\n",
        "# Standardize features using training data statistics (from X_train only)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
        "X_test_damaged_scaled = scaler.transform(X_test_damaged)\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Define a Model Builder for Hyperparameter Tuning (VAE)\n",
        "# =============================================================================\n",
        "def build_vae(hp):\n",
        "    # Choose hyperparameters\n",
        "    latent_dim = hp.Choice('latent_dim', values=[8, 16, 32])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    l2_reg = hp.Float('l2_reg', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    beta = hp.Float('beta', min_value=0.05, max_value=0.5, step=0.05)\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = layers.Input(shape=(input_dim,), name=\"encoder_input\")\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(encoder_inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    # \"z_mean\" will be used later as the latent representation.\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = layers.Input(shape=(latent_dim,), name=\"decoder_input\")\n",
        "    x_dec = layers.Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(decoder_inputs)\n",
        "    x_dec = layers.BatchNormalization()(x_dec)\n",
        "    x_dec = layers.Dropout(dropout_rate)(x_dec)\n",
        "    x_dec = layers.Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x_dec)\n",
        "    x_dec = layers.BatchNormalization()(x_dec)\n",
        "    x_dec = layers.Dropout(dropout_rate)(x_dec)\n",
        "    x_dec = layers.Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x_dec)\n",
        "    x_dec = layers.BatchNormalization()(x_dec)\n",
        "    x_dec = layers.Dropout(dropout_rate)(x_dec)\n",
        "    decoder_outputs = layers.Dense(input_dim, activation='linear')(x_dec)\n",
        "\n",
        "    # Build decoder model (used for prediction later)\n",
        "    decoder = Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "    # Custom loss layer\n",
        "    class VAELossLayer(layers.Layer):\n",
        "        def __init__(self, beta=beta, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.beta = beta\n",
        "\n",
        "        def call(self, inputs):\n",
        "            x, x_decoded, z_mean, z_log_var = inputs\n",
        "            reconstruction_loss = tf.reduce_mean(tf.square(x - x_decoded), axis=1) * input_dim\n",
        "            kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
        "            total_loss = tf.reduce_mean(reconstruction_loss + self.beta * kl_loss)\n",
        "            self.add_loss(total_loss)\n",
        "            return x_decoded\n",
        "\n",
        "    vae_outputs = decoder(z)\n",
        "    vae_loss_output = VAELossLayer(name='vae_loss')([encoder_inputs, vae_outputs, z_mean, z_log_var])\n",
        "    vae = Model(encoder_inputs, vae_loss_output, name=\"vae\")\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=hp.Float('learning_rate', 1e-4, 1e-3, sampling='LOG')\n",
        "    ))\n",
        "    return vae\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Hyperparameter Tuning Using Keras Tuner (VAE)\n",
        "# =============================================================================\n",
        "tuner = kt.RandomSearch(\n",
        "    build_vae,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,  # Adjust the number of trials as needed\n",
        "    executions_per_trial=1,\n",
        "    directory='vae_tuner_dir',\n",
        "    project_name='anomaly_detection_tuning'\n",
        ")\n",
        "\n",
        "# Use the 20% validation split (from the training set) for tuning\n",
        "tuner.search(X_train_scaled, X_train_scaled,\n",
        "             epochs=100,\n",
        "             batch_size=64,\n",
        "             validation_data=(X_val_scaled, X_val_scaled),\n",
        "             callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=0.001)],\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve the best model and hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters:\")\n",
        "print(f\"  latent_dim: {best_hps.get('latent_dim')}\")\n",
        "print(f\"  dropout_rate: {best_hps.get('dropout_rate')}\")\n",
        "print(f\"  l2_reg: {best_hps.get('l2_reg')}\")\n",
        "print(f\"  beta: {best_hps.get('beta')}\")\n",
        "print(f\"  learning_rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# Build the VAE model with best hyperparameters and train it\n",
        "best_model = build_vae(best_hps)\n",
        "early_stop = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True, min_delta=0.001)\n",
        "history = best_model.fit(X_train_scaled, X_train_scaled,\n",
        "                         epochs=500,\n",
        "                         batch_size=64,\n",
        "                         validation_data=(X_val_scaled, X_val_scaled),\n",
        "                         callbacks=[early_stop],\n",
        "                         verbose=1)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Compute VAE Reconstruction Errors (for reference)\n",
        "# =============================================================================\n",
        "train_recon = best_model.predict(X_train_scaled)\n",
        "train_errors = np.mean(np.square(X_train_scaled - train_recon), axis=1)\n",
        "\n",
        "val_recon = best_model.predict(X_val_scaled)\n",
        "val_errors = np.mean(np.square(X_val_scaled - val_recon), axis=1)\n",
        "\n",
        "test_normal_recon = best_model.predict(X_test_normal_scaled)\n",
        "test_normal_errors = np.mean(np.square(X_test_normal_scaled - test_normal_recon), axis=1)\n",
        "\n",
        "test_damaged_recon = best_model.predict(X_test_damaged_scaled)\n",
        "test_damaged_errors = np.mean(np.square(X_test_damaged_scaled - test_damaged_recon), axis=1)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4.5: Extract Latent Representations Using the Trained VAE Encoder\n",
        "# =============================================================================\n",
        "encoder = Model(inputs=best_model.input, outputs=best_model.get_layer(\"z_mean\").output)\n",
        "\n",
        "latent_train = encoder.predict(X_train_scaled)\n",
        "latent_val = encoder.predict(X_val_scaled)\n",
        "latent_test_normal = encoder.predict(X_test_normal_scaled)\n",
        "latent_test_damaged = encoder.predict(X_test_damaged_scaled)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: One-Class SVM Model on the Latent Representations\n",
        "# =============================================================================\n",
        "param_grid = {\n",
        "    'nu': [0.01, 0.05, 0.1],\n",
        "    'gamma': ['scale', 0.01, 0.001]\n",
        "}\n",
        "\n",
        "best_ocsvm = None\n",
        "best_outlier_fraction = np.inf\n",
        "best_params = {}\n",
        "\n",
        "for nu in param_grid['nu']:\n",
        "    for gamma in param_grid['gamma']:\n",
        "        ocsvm = OneClassSVM(nu=nu, kernel='rbf', gamma=gamma)\n",
        "        ocsvm.fit(latent_train)\n",
        "        preds = ocsvm.predict(latent_train)  # +1 for inliers, -1 for outliers\n",
        "        outlier_fraction = np.mean(preds == -1)\n",
        "        if outlier_fraction < best_outlier_fraction:\n",
        "            best_outlier_fraction = outlier_fraction\n",
        "            best_params = {'nu': nu, 'gamma': gamma}\n",
        "            best_ocsvm = ocsvm\n",
        "\n",
        "print(\"Best One-Class SVM parameters found:\")\n",
        "print(best_params)\n",
        "print(f\"Fraction of outliers on training data: {best_outlier_fraction:.4f}\")\n",
        "\n",
        "ocsvm_model = OneClassSVM(nu=best_params['nu'], kernel='rbf', gamma=best_params['gamma'])\n",
        "ocsvm_model.fit(latent_train)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Compute Anomaly Scores Using the One-Class SVM\n",
        "# =============================================================================\n",
        "train_scores = ocsvm_model.decision_function(latent_train)\n",
        "val_scores = ocsvm_model.decision_function(latent_val)\n",
        "test_normal_scores = ocsvm_model.decision_function(latent_test_normal)\n",
        "test_damaged_scores = ocsvm_model.decision_function(latent_test_damaged)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6.5: Normalize Anomaly Scores for Fusion (Both Methods are Computed)\n",
        "# =============================================================================\n",
        "# (Although the VAE reconstruction errors are computed, we will now ignore them.)\n",
        "# Normalize VAE reconstruction errors using training data stats\n",
        "min_train_vae = np.min(train_errors)\n",
        "max_train_vae = np.max(train_errors)\n",
        "norm_train_vae = (train_errors - min_train_vae) / (max_train_vae - min_train_vae)\n",
        "norm_val_vae = (val_errors - min_train_vae) / (max_train_vae - min_train_vae)\n",
        "norm_test_normal_vae = (test_normal_errors - min_train_vae) / (max_train_vae - min_train_vae)\n",
        "norm_test_damaged_vae = (test_damaged_errors - min_train_vae) / (max_train_vae - min_train_vae)\n",
        "\n",
        "# Convert OCSVM scores to anomaly measures (higher anomaly = more positive)\n",
        "train_ocsvm_anomaly = -train_scores\n",
        "val_ocsvm_anomaly = -val_scores\n",
        "test_normal_ocsvm_anomaly = -test_normal_scores\n",
        "test_damaged_ocsvm_anomaly = -test_damaged_scores\n",
        "\n",
        "min_train_ocsvm = np.min(train_ocsvm_anomaly)\n",
        "max_train_ocsvm = np.max(train_ocsvm_anomaly)\n",
        "norm_train_ocsvm = (train_ocsvm_anomaly - min_train_ocsvm) / (max_train_ocsvm - min_train_ocsvm)\n",
        "norm_val_ocsvm = (val_ocsvm_anomaly - min_train_ocsvm) / (max_train_ocsvm - min_train_ocsvm)\n",
        "norm_test_normal_ocsvm = (test_normal_ocsvm_anomaly - min_train_ocsvm) / (max_train_ocsvm - min_train_ocsvm)\n",
        "norm_test_damaged_ocsvm = (test_damaged_ocsvm_anomaly - min_train_ocsvm) / (max_train_ocsvm - min_train_ocsvm)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: Compute Composite Anomaly Scores Using Only the One-Class SVM Score\n",
        "# =============================================================================\n",
        "# In this modified version, we use only the normalized One-Class SVM anomaly measure.\n",
        "composite_train = norm_train_ocsvm\n",
        "composite_val   = norm_val_ocsvm\n",
        "composite_test_normal = norm_test_normal_ocsvm\n",
        "composite_test_damaged = norm_test_damaged_ocsvm\n",
        "\n",
        "# Compute EVT threshold on training composite scores using a fixed tail probability (e.g., 0.95).\n",
        "u = np.percentile(composite_train, 90)\n",
        "excesses = composite_train[composite_train > u] - u\n",
        "params = genpareto.fit(excesses)\n",
        "tail_prob = 0.95\n",
        "threshold_evt = u + genpareto.ppf(tail_prob, *params)\n",
        "print(f\"Final EVT-calibrated composite threshold: {threshold_evt:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: Combine Composite Anomaly Scores into a Single DataFrame\n",
        "# =============================================================================\n",
        "df_train = pd.DataFrame({\n",
        "    \"Composite_Anomaly_Score\": composite_train,\n",
        "    \"Dataset\": \"Training - Normal\"\n",
        "})\n",
        "df_val = pd.DataFrame({\n",
        "    \"Composite_Anomaly_Score\": composite_val,\n",
        "    \"Dataset\": \"Validation - Normal\"\n",
        "})\n",
        "df_test_normal = pd.DataFrame({\n",
        "    \"Composite_Anomaly_Score\": composite_test_normal,\n",
        "    \"Dataset\": \"Inspection - Normal\"\n",
        "})\n",
        "df_test_damaged = pd.DataFrame({\n",
        "    \"Composite_Anomaly_Score\": composite_test_damaged,\n",
        "    \"Dataset\": \"Inspection - Damaged\"\n",
        "})\n",
        "all_scores_df = pd.concat([df_train, df_val, df_test_normal, df_test_damaged], ignore_index=True)\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Mean_Train_Composite_Score\": [np.mean(composite_train)],\n",
        "    \"Std_Train_Composite_Score\": [np.std(composite_train)],\n",
        "    \"Mean_Val_Composite_Score\": [np.mean(composite_val)],\n",
        "    \"Std_Val_Composite_Score\": [np.std(composite_val)],\n",
        "    \"Mean_Test_Normal_Composite_Score\": [np.mean(composite_test_normal)],\n",
        "    \"Std_Test_Normal_Composite_Score\": [np.std(composite_test_normal)],\n",
        "    \"Mean_Test_Damaged_Composite_Score\": [np.mean(composite_test_damaged)],\n",
        "    \"Std_Test_Damaged_Composite_Score\": [np.std(composite_test_damaged)]\n",
        "})\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 9: Save Results to an Excel File with Two Sheets\n",
        "# =============================================================================\n",
        "output_excel_file = \"AnomalyDetection_Tuned_Results.xlsx\"\n",
        "with pd.ExcelWriter(output_excel_file) as writer:\n",
        "    all_scores_df.to_excel(writer, sheet_name=\"Composite_Anomaly_Scores\", index=False)\n",
        "    summary_df.to_excel(writer, sheet_name=\"Composite_Score_Summary\", index=False)\n",
        "\n",
        "print(f\"Results saved to '{output_excel_file}'.\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 10: Visualization (Optional)\n",
        "# =============================================================================\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(range(len(composite_train)), composite_train, s=3, label=\"Training - Normal\")\n",
        "plt.scatter(len(composite_train) + np.arange(len(composite_val)), composite_val,\n",
        "            s=3, c='green', label=\"Validation - Normal\")\n",
        "plt.scatter(len(composite_train) + len(composite_val) + np.arange(len(composite_test_normal)),\n",
        "            composite_test_normal, s=3, c='orange', label=\"Inspection - Normal\")\n",
        "plt.scatter(len(composite_train) + len(composite_val) + len(composite_test_normal) + np.arange(len(composite_test_damaged)),\n",
        "            composite_test_damaged, s=3, c='red', label=\"Inspection - Damaged\")\n",
        "# Plot the EVT-calibrated threshold.\n",
        "plt.axhline(threshold_evt, c='black', linestyle='--', label=\"EVT Threshold\")\n",
        "plt.ylabel(\"Composite Anomaly Score\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.title(\"Anomaly Detection Performance (Using One-Class SVM Only)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 11: Parameter Sensitivity Analysis (VAE + OCSVM)\n",
        "# =============================================================================\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Extract VAE Tuning Results\n",
        "# -------------------------------\n",
        "vae_results = []\n",
        "for trial in tuner.oracle.trials.values():\n",
        "    hp_values = trial.hyperparameters.values\n",
        "    val_loss = trial.score  # Validation loss for this trial\n",
        "    vae_results.append({\n",
        "        'latent_dim': hp_values.get('latent_dim'),\n",
        "        'beta': hp_values.get('beta'),\n",
        "        'dropout_rate': hp_values.get('dropout_rate'),\n",
        "        'l2_reg': hp_values.get('l2_reg'),\n",
        "        'learning_rate': hp_values.get('learning_rate'),\n",
        "        'val_loss': val_loss\n",
        "    })\n",
        "\n",
        "vae_results_df = pd.DataFrame(vae_results)\n",
        "vae_results_df.to_excel(\"VAE_Hyperparameter_Sensitivity.xlsx\", index=False)\n",
        "print(\"Saved VAE hyperparameter sensitivity results to 'VAE_Hyperparameter_Sensitivity.xlsx'\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Extract OCSVM Parameter Sensitivity\n",
        "# -------------------------------\n",
        "ocsvm_results = []\n",
        "for nu in param_grid['nu']:\n",
        "    for gamma in param_grid['gamma']:\n",
        "        ocsvm = OneClassSVM(nu=nu, kernel='rbf', gamma=gamma)\n",
        "        ocsvm.fit(latent_train)\n",
        "        preds = ocsvm.predict(latent_train)\n",
        "        outlier_fraction = np.mean(preds == -1)\n",
        "        ocsvm_results.append({\n",
        "            'nu': nu,\n",
        "            'gamma': gamma,\n",
        "            'outlier_fraction': outlier_fraction\n",
        "        })\n",
        "\n",
        "ocsvm_results_df = pd.DataFrame(ocsvm_results)\n",
        "ocsvm_results_df.to_excel(\"OCSVM_Parameter_Sensitivity.xlsx\", index=False)\n",
        "print(\"Saved OCSVM parameter sensitivity results to 'OCSVM_Parameter_Sensitivity.xlsx'\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Plot Sensitivity Analysis\n",
        "# -------------------------------\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "# (a) Latent Dimension\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.boxplot(x='latent_dim', y='val_loss', data=vae_results_df, palette='Blues')\n",
        "plt.title(\"(a) Latent Dimension vs Validation Loss\")\n",
        "plt.xlabel(\"Latent Dimension\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "\n",
        "# (b) Beta (KL Divergence Weight)\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.scatterplot(x='beta', y='val_loss', data=vae_results_df, s=60, color='darkorange')\n",
        "plt.title(\"(b) β (KL Weight) vs Validation Loss\")\n",
        "plt.xlabel(\"β (KL Divergence Weight)\")\n",
        "plt.ylabel(\"Validation Loss\")\n",
        "\n",
        "# (c) OCSVM Parameter Sensitivity Heatmap\n",
        "plt.subplot(1, 3, 3)\n",
        "pivot_table = ocsvm_results_df.pivot(index='nu', columns='gamma', values='outlier_fraction')\n",
        "sns.heatmap(pivot_table, annot=True, cmap='Reds', fmt=\".3f\")\n",
        "plt.title(\"(c) OCSVM Parameter Sensitivity\")\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Nu\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}