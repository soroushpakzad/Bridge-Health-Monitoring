{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jxl0SysKwjxq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from scipy.stats import genpareto\n",
        "from sklearn.ensemble import IsolationForest  # Using Isolation Forest for anomaly detection\n",
        "!pip install keras-tuner\n",
        "import kerastuner as kt  # make sure to install keras-tuner (pip install keras-tuner)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Load and Split the Dataset\n",
        "# =============================================================================\n",
        "# Load the full training data (Normal Condition)\n",
        "X_train_full = pd.read_excel(io=\"Freq_Z24N.xlsx\", sheet_name=\"Train Set\", header=0, engine=\"openpyxl\").values\n",
        "\n",
        "# Split the training set into training (80%) and validation (20%) sets\n",
        "X_train, X_val = train_test_split(X_train_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the test set, which remains unseen. It contains two parts:\n",
        "#   - first part: Inspection - Normal Condition (samples 1 to 510)\n",
        "#   - second part: Inspection - Damaged Condition (samples 511 to end)\n",
        "X_test_full = pd.read_excel(io=\"Freq_Z24N.xlsx\", sheet_name=\"Test Set\", header=0, engine=\"openpyxl\").values\n",
        "X_test_normal = X_test_full[:510]    # Inspection - Normal Condition\n",
        "X_test_damaged = X_test_full[510:]     # Inspection - Damaged Condition\n",
        "\n",
        "# Standardize features using training data statistics (from X_train only)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_normal_scaled = scaler.transform(X_test_normal)\n",
        "X_test_damaged_scaled = scaler.transform(X_test_damaged)\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Define a Model Builder for Hyperparameter Tuning (VAE)\n",
        "# =============================================================================\n",
        "def build_vae(hp):\n",
        "    # Choose hyperparameters\n",
        "    latent_dim = hp.Choice('latent_dim', values=[8, 16, 32])\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
        "    l2_reg = hp.Float('l2_reg', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    beta = hp.Float('beta', min_value=0.05, max_value=0.5, step=0.05)\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = layers.Input(shape=(input_dim,), name=\"encoder_input\")\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(encoder_inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    # \"z_mean\" will be used later as the latent representation.\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "    z = layers.Lambda(sampling, name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = layers.Input(shape=(latent_dim,), name=\"decoder_input\")\n",
        "    x_dec = layers.Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(decoder_inputs)\n",
        "    x_dec = layers.BatchNormalization()(x_dec)\n",
        "    x_dec = layers.Dropout(dropout_rate)(x_dec)\n",
        "    x_dec = layers.Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x_dec)\n",
        "    x_dec = layers.BatchNormalization()(x_dec)\n",
        "    x_dec = layers.Dropout(dropout_rate)(x_dec)\n",
        "    x_dec = layers.Dense(256, activation='relu', kernel_regularizer=l2(l2_reg))(x_dec)\n",
        "    x_dec = layers.BatchNormalization()(x_dec)\n",
        "    x_dec = layers.Dropout(dropout_rate)(x_dec)\n",
        "    decoder_outputs = layers.Dense(input_dim, activation='linear')(x_dec)\n",
        "\n",
        "    # Build decoder model (used for prediction later)\n",
        "    decoder = Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
        "\n",
        "    # Custom loss layer\n",
        "    class VAELossLayer(layers.Layer):\n",
        "        def __init__(self, beta=beta, **kwargs):\n",
        "            super().__init__(**kwargs)\n",
        "            self.beta = beta\n",
        "\n",
        "        def call(self, inputs):\n",
        "            x, x_decoded, z_mean, z_log_var = inputs\n",
        "            reconstruction_loss = tf.reduce_mean(tf.square(x - x_decoded), axis=1) * input_dim\n",
        "            kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
        "            total_loss = tf.reduce_mean(reconstruction_loss + self.beta * kl_loss)\n",
        "            self.add_loss(total_loss)\n",
        "            return x_decoded\n",
        "\n",
        "    vae_outputs = decoder(z)\n",
        "    vae_loss_output = VAELossLayer(name='vae_loss')([encoder_inputs, vae_outputs, z_mean, z_log_var])\n",
        "    vae = Model(encoder_inputs, vae_loss_output, name=\"vae\")\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=hp.Float('learning_rate', 1e-4, 1e-3, sampling='LOG')\n",
        "    ))\n",
        "    return vae\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Hyperparameter Tuning Using Keras Tuner (VAE)\n",
        "# =============================================================================\n",
        "tuner = kt.RandomSearch(\n",
        "    build_vae,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,  # Adjust the number of trials as needed\n",
        "    executions_per_trial=1,\n",
        "    directory='vae_tuner_dir',\n",
        "    project_name='anomaly_detection_tuning'\n",
        ")\n",
        "\n",
        "# Use the 20% validation split (from the training set) for tuning\n",
        "tuner.search(X_train_scaled, X_train_scaled,\n",
        "             epochs=100,\n",
        "             batch_size=64,\n",
        "             validation_data=(X_val_scaled, X_val_scaled),\n",
        "             callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=0.001)],\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve the best model and hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters:\")\n",
        "print(f\"  latent_dim: {best_hps.get('latent_dim')}\")\n",
        "print(f\"  dropout_rate: {best_hps.get('dropout_rate')}\")\n",
        "print(f\"  l2_reg: {best_hps.get('l2_reg')}\")\n",
        "print(f\"  beta: {best_hps.get('beta')}\")\n",
        "print(f\"  learning_rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# Build the VAE model with best hyperparameters and train it\n",
        "best_model = build_vae(best_hps)\n",
        "early_stop = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True, min_delta=0.001)\n",
        "history = best_model.fit(X_train_scaled, X_train_scaled,\n",
        "                         epochs=500,\n",
        "                         batch_size=64,\n",
        "                         validation_data=(X_val_scaled, X_val_scaled),\n",
        "                         callbacks=[early_stop],\n",
        "                         verbose=1)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Extract Latent Representations Using the Trained VAE Encoder\n",
        "# =============================================================================\n",
        "encoder = Model(inputs=best_model.input, outputs=best_model.get_layer(\"z_mean\").output)\n",
        "\n",
        "latent_train = encoder.predict(X_train_scaled)\n",
        "latent_val = encoder.predict(X_val_scaled)\n",
        "latent_test_normal = encoder.predict(X_test_normal_scaled)\n",
        "latent_test_damaged = encoder.predict(X_test_damaged_scaled)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: Isolation Forest Model on the Latent Representations\n",
        "# =============================================================================\n",
        "# Define a parameter grid for Isolation Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_samples': [0.5, 0.8, 'auto'],\n",
        "    'contamination': [0.01, 0.05, 0.1]\n",
        "}\n",
        "\n",
        "best_if = None\n",
        "best_outlier_fraction = np.inf\n",
        "best_params = {}\n",
        "\n",
        "for n_estimators in param_grid['n_estimators']:\n",
        "    for max_samples in param_grid['max_samples']:\n",
        "        for contamination in param_grid['contamination']:\n",
        "            if_model = IsolationForest(n_estimators=n_estimators,\n",
        "                                       max_samples=max_samples,\n",
        "                                       contamination=contamination,\n",
        "                                       random_state=42)\n",
        "            if_model.fit(latent_train)\n",
        "            preds = if_model.predict(latent_train)  # +1 for inliers, -1 for outliers\n",
        "            outlier_fraction = np.mean(preds == -1)\n",
        "            if outlier_fraction < best_outlier_fraction:\n",
        "                best_outlier_fraction = outlier_fraction\n",
        "                best_params = {'n_estimators': n_estimators, 'max_samples': max_samples, 'contamination': contamination}\n",
        "                best_if = if_model\n",
        "\n",
        "print(\"Best Isolation Forest parameters found:\")\n",
        "print(best_params)\n",
        "print(f\"Fraction of outliers on training data: {best_outlier_fraction:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: Compute Anomaly Scores Using the Isolation Forest Model Only\n",
        "# =============================================================================\n",
        "# The decision_function returns a score where higher values indicate normality.\n",
        "# We convert these to anomaly measures by taking the negative, so that higher values indicate more anomalous.\n",
        "train_scores = best_if.decision_function(latent_train)\n",
        "val_scores = best_if.decision_function(latent_val)\n",
        "test_normal_scores = best_if.decision_function(latent_test_normal)\n",
        "test_damaged_scores = best_if.decision_function(latent_test_damaged)\n",
        "\n",
        "train_if_anomaly = -train_scores\n",
        "val_if_anomaly = -val_scores\n",
        "test_normal_if_anomaly = -test_normal_scores\n",
        "test_damaged_if_anomaly = -test_damaged_scores\n",
        "\n",
        "# Normalize Isolation Forest anomaly scores using training data statistics\n",
        "min_train_if = np.min(train_if_anomaly)\n",
        "max_train_if = np.max(train_if_anomaly)\n",
        "norm_train_if = (train_if_anomaly - min_train_if) / (max_train_if - min_train_if)\n",
        "norm_val_if = (val_if_anomaly - min_train_if) / (max_train_if - min_train_if)\n",
        "norm_test_normal_if = (test_normal_if_anomaly - min_train_if) / (max_train_if - min_train_if)\n",
        "norm_test_damaged_if = (test_damaged_if_anomaly - min_train_if) / (max_train_if - min_train_if)\n",
        "\n",
        "# =============================================================================\n",
        "# (Optional) STEP 7: EVT Calibration on Isolation Forest Anomaly Scores\n",
        "# =============================================================================\n",
        "# Here we set a threshold using Extreme Value Theory (EVT) on the normalized anomaly scores.\n",
        "u = np.percentile(norm_train_if, 90)\n",
        "excesses = norm_train_if[norm_train_if > u] - u\n",
        "params = genpareto.fit(excesses)\n",
        "tail_probability = 0.95  # Adjust as desired\n",
        "threshold_evt = u + genpareto.ppf(tail_probability, *params)\n",
        "print(f\"Final EVT-calibrated Isolation Forest threshold: {threshold_evt:.4f}\")\n",
        "\n",
        "# For your final anomaly score, we will use the normalized Isolation Forest anomaly scores directly.\n",
        "final_train_scores = norm_train_if\n",
        "final_val_scores = norm_val_if\n",
        "final_test_normal_scores = norm_test_normal_if\n",
        "final_test_damaged_scores = norm_test_damaged_if\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: Combine Isolation Forest Anomaly Scores into a Single DataFrame\n",
        "# =============================================================================\n",
        "df_train = pd.DataFrame({\n",
        "    \"IF_Anomaly_Score\": final_train_scores,\n",
        "    \"Dataset\": \"Training - Normal\"\n",
        "})\n",
        "df_val = pd.DataFrame({\n",
        "    \"IF_Anomaly_Score\": final_val_scores,\n",
        "    \"Dataset\": \"Validation - Normal\"\n",
        "})\n",
        "df_test_normal = pd.DataFrame({\n",
        "    \"IF_Anomaly_Score\": final_test_normal_scores,\n",
        "    \"Dataset\": \"Inspection - Normal\"\n",
        "})\n",
        "df_test_damaged = pd.DataFrame({\n",
        "    \"IF_Anomaly_Score\": final_test_damaged_scores,\n",
        "    \"Dataset\": \"Inspection - Damaged\"\n",
        "})\n",
        "all_scores_df = pd.concat([df_train, df_val, df_test_normal, df_test_damaged], ignore_index=True)\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Mean_Train_IF_Score\": [np.mean(final_train_scores)],\n",
        "    \"Std_Train_IF_Score\": [np.std(final_train_scores)],\n",
        "    \"Mean_Val_IF_Score\": [np.mean(final_val_scores)],\n",
        "    \"Std_Val_IF_Score\": [np.std(final_val_scores)],\n",
        "    \"Mean_Test_Normal_IF_Score\": [np.mean(final_test_normal_scores)],\n",
        "    \"Std_Test_Normal_IF_Score\": [np.std(final_test_normal_scores)],\n",
        "    \"Mean_Test_Damaged_IF_Score\": [np.mean(final_test_damaged_scores)],\n",
        "    \"Std_Test_Damaged_IF_Score\": [np.std(final_test_damaged_scores)]\n",
        "})\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 9: Save Results to an Excel File with Two Sheets\n",
        "# =============================================================================\n",
        "output_excel_file = \"IF_AnomalyDetection_Results.xlsx\"\n",
        "with pd.ExcelWriter(output_excel_file) as writer:\n",
        "    all_scores_df.to_excel(writer, sheet_name=\"IF_Anomaly_Scores\", index=False)\n",
        "    summary_df.to_excel(writer, sheet_name=\"IF_Score_Summary\", index=False)\n",
        "\n",
        "print(f\"Results saved to '{output_excel_file}'.\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 10: Visualization (Optional)\n",
        "# =============================================================================\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(range(len(final_train_scores)), final_train_scores, s=3, label=\"Training - Normal\")\n",
        "plt.scatter(len(final_train_scores) + np.arange(len(final_val_scores)), final_val_scores,\n",
        "            s=3, c='green', label=\"Validation - Normal\")\n",
        "plt.scatter(len(final_train_scores) + len(final_val_scores) + np.arange(len(final_test_normal_scores)),\n",
        "            final_test_normal_scores, s=3, c='orange', label=\"Inspection - Normal\")\n",
        "plt.scatter(len(final_train_scores) + len(final_val_scores) + len(final_test_normal_scores) + np.arange(len(final_test_damaged_scores)),\n",
        "            final_test_damaged_scores, s=3, c='red', label=\"Inspection - Damaged\")\n",
        "# Plot the EVT-calibrated threshold.\n",
        "plt.axhline(threshold_evt, c='black', linestyle='--', label=\"EVT Threshold\")\n",
        "plt.ylabel(\"Normalized Isolation Forest Anomaly Score\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.title(\"Isolation Forest-based Anomaly Detection\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}